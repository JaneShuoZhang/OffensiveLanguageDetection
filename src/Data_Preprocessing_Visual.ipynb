{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordsegment import load, segment\n",
    "import emoji\n",
    "from preprocessing import process_single_tweet, process_train_data, process_test_data, process_trial_data\n",
    "from utils import load_train_data, load_test_data_a, load_trial_data\n",
    "from feature_embedding import get_all_character_ngrams_of_sentence, build_ngrams_dataset, build_glove_featurized_dataset\n",
    "from ml_classifiers import MLDetector, run_logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data: 13240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER @USER Go home youâ€™re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet subtask_a\n",
       "0  @USER She should ask a few native Americans wh...       OFF\n",
       "1  @USER @USER Go home youâ€™re drunk!!! @USER #MAG...       OFF\n",
       "2  Amazon is investigating Chinese employees who ...       NOT\n",
       "3  @USER Someone should'veTaken\" this piece of sh...       OFF\n",
       "4  @USER @USER Obama wanted liberals &amp; illega...       NOT"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_train_data()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result from NLTK twitter tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL\n",
      "['@USER', '@USER', 'Go', 'home', 'you', 'â€™', 're', 'drunk', '!', '!', '!', '@USER', '#MAGA', '#Trump2020', 'ðŸ‘Š', 'ðŸ‡º', 'ðŸ‡¸', 'ðŸ‘Š', 'URL']\n"
     ]
    }
   ],
   "source": [
    "example = train_data[\"tweet\"][1]\n",
    "print(example)\n",
    "twt_tokenizer = TweetTokenizer()\n",
    "tokenized = twt_tokenizer.tokenize(example)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result from our pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL\n",
      "Creating dictionary...\n",
      "total words processed: 223118\n",
      "total unique words in corpus: 223118\n",
      "total items in dictionary (corpus words and deletions): 6778287\n",
      "  edit distance for deletions: 3\n",
      "  length of longest word in corpus: 15\n",
      "go home you be drink ! ! ! magna trump 2020 oncoming fist unite state oncoming fist\n"
     ]
    }
   ],
   "source": [
    "print(example)\n",
    "print(process_single_tweet(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of test data A: 860\n",
      "Processed data already exists. Direct load it.\n",
      "number of processed data: 860\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OFF</td>\n",
       "      <td>who be q where the server dump nike dec las fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOT</td>\n",
       "      <td>constitution day be revere by conservatives ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOT</td>\n",
       "      <td>foxnews era magna pot trump and amendment rnc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOT</td>\n",
       "      <td>watch boomer get the news that she be still up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OFF</td>\n",
       "      <td>no saharan unity demo to oppose the far right ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subtask_a                                              tweet\n",
       "0       OFF  who be q where the server dump nike dec las fi...\n",
       "1       NOT  constitution day be revere by conservatives ha...\n",
       "2       NOT  foxnews era magna pot trump and amendment rnc ...\n",
       "3       NOT  watch boomer get the news that she be still up...\n",
       "4       OFF  no saharan unity demo to oppose the far right ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_test_data_a()\n",
    "test_data = process_test_data(test_data)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Character n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhhhh so funny !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'!': 1,\n",
       " 'f': 1,\n",
       " 'fu': 1,\n",
       " 'fun': 1,\n",
       " 'funn': 1,\n",
       " 'h': 5,\n",
       " 'hh': 4,\n",
       " 'hhh': 3,\n",
       " 'hhhh': 2,\n",
       " 'n': 2,\n",
       " 'nn': 1,\n",
       " 'nny': 1,\n",
       " 'ny': 1,\n",
       " 'o': 1,\n",
       " 's': 1,\n",
       " 'so': 1,\n",
       " 'u': 1,\n",
       " 'un': 1,\n",
       " 'unn': 1,\n",
       " 'unny': 1,\n",
       " 'y': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"hhhhh so funny !\"\n",
    "print(example)\n",
    "get_all_character_ngrams_of_sentence(example)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build training data based on character n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "Shape of X is (10, 892)\n",
      "[1, 0, 0, 0, 1, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "sample_train = test_data.head(10)\n",
    "sample_test = test_data.tail(5)\n",
    "train_set = build_ngrams_dataset(sample_train)\n",
    "print(train_set['X'])\n",
    "print(\"Shape of X is {}\".format(train_set['X'].shape))\n",
    "print(train_set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build test data using the same vectorizer when building training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 2. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = train_set['vectorizer']\n",
    "test_set = build_ngrams_dataset(sample_test, vectorizer=vectorizer)\n",
    "print(test_set['X'])\n",
    "print(test_set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a 25d embedding for illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe loaded. Vocabulary size: 1193514\n",
      "Shape of X is (10, 25)\n",
      "[1, 0, 0, 0, 1, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_set2 = build_glove_featurized_dataset(sample_train, 25)\n",
    "print(\"Shape of X is {}\".format(train_set2['X'].shape))\n",
    "print(train_set2['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression + character n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate and test, we use true test dataset as our train dataset, and true trial dataset as our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trial data: 320\n",
      "Processed data already exists. Direct load it.\n",
      "number of processed data: 320\n",
      "number of test data A: 860\n",
      "Processed data already exists. Direct load it.\n",
      "number of processed data: 860\n",
      "Train data: 860; Test data: 320.\n"
     ]
    }
   ],
   "source": [
    "trial_data = load_trial_data()\n",
    "trial_data = process_trial_data(trial_data)\n",
    "test_data = load_test_data_a()\n",
    "test_data = process_test_data(test_data)\n",
    "# Change to train and test set.\n",
    "train_data = test_data\n",
    "test_data = trial_data\n",
    "print(\"Train data: {}; Test data: {}.\".format(train_data.shape[0], test_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training X, y, and testing X, y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X: (860, 10569)\n",
      "Shape of test_X: (320, 10569)\n"
     ]
    }
   ],
   "source": [
    "train_set_ngram = build_ngrams_dataset(train_data)\n",
    "train_X = train_set_ngram['X']\n",
    "train_y = train_set_ngram['y']\n",
    "vectorizer = train_set_ngram['vectorizer']\n",
    "print(\"Shape of train_X: {}\".format(train_X.shape))\n",
    "test_set_ngram = build_ngrams_dataset(test_data, vectorizer=vectorizer)\n",
    "test_X = test_set_ngram['X']\n",
    "test_y = test_set_ngram['y']\n",
    "print(\"Shape of test_X: {}\".format(test_X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning and select best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] penalty=l2 ......................................................\n",
      "[CV] penalty=l2 ......................................................\n"
     ]
    }
   ],
   "source": [
    "lr_classifier = MLDetector('LR')\n",
    "params_set = {'penalty': ['l2']}\n",
    "lr_tune = lr_classifier.hyper_tune(test_X, test_y, params_set, best_only=False)\n",
    "print('Hyperparameter Tuning: ', lr_tune)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
